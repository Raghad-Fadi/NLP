{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17628cd-dd81-4897-b727-76f8fa64a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7c4958-a261-4479-8ca8-39be7dc528ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>10061</td>\n",
       "      <td>2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>10062</td>\n",
       "      <td>Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>10063</td>\n",
       "      <td>Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>10064</td>\n",
       "      <td>ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>10065</td>\n",
       "      <td>AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10065 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                              TWEET     LABEL\n",
       "0          1            Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..      none\n",
       "1          2  Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...     anger\n",
       "2          3                         ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD   sadness\n",
       "3          4  Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...       joy\n",
       "4          5  Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...      none\n",
       "...      ...                                                ...       ...\n",
       "10060  10061  2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...   sadness\n",
       "10061  10062                  Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤  surprise\n",
       "10062  10063    Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ      none\n",
       "10063  10064    ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³       joy\n",
       "10064  10065  AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...     anger\n",
       "\n",
       "[10065 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Emotional-Tone-Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ad2b8-a4bd-47d6-82d3-638f17aeb746",
   "metadata": {},
   "source": [
    "# **PART ONE:**\n",
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84aad15f-9b02-43e8-a15f-a89d024a324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "def normalize_repeated_letters(text):\n",
    "\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = normalize_repeated_letters(text)\n",
    "\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    text = re.sub(r'Ù‰', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø¤', 'Ùˆ', text)\n",
    "    text = re.sub(r'Ø¦', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
    "\n",
    "    \n",
    "    text = strip_tashkeel(strip_tatweel(text))\n",
    "\n",
    "\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "    \n",
    "df['clean_text'] = df[' TWEET'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e3670a-3480-4c1b-9f7d-07ffce0fce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ø§Ù„ÙƒÙ„ÙŠÙ‡\n",
       "1        Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ Ù„Ø³Ù‡ Ø§Ù‚Ù„ Ù†Ù...\n",
       "2                                    ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨\n",
       "3        Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª Ø­Ø±Ø§Ø³Ù‡ Ø§Ù„Ù…Ø±Ù…ÙŠ\n",
       "4        Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ Ù…ÙƒØ§...\n",
       "                               ...                        \n",
       "10060    ÙŠÙ„Ø§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´ÙˆÙŠÙ‡ ÙˆÙ†Ø¨Ø¯Ø§ ...\n",
       "10061                                            Ø§ÙŠÙ‡ Ø§ÙˆØ²ÙŠÙ„\n",
       "10062        Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯\n",
       "10063      ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³\n",
       "10064    ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹Ø§Ù„Ù… ØªØ¬Ù…Ø¹Øª Ø§Ù„Ø§ÙŠØ±Ø§Ù†...\n",
       "Name: clean_text, Length: 10065, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3e70d-59d0-4d4d-bd24-7234babccc50",
   "metadata": {},
   "source": [
    "## Label Encoding and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdadc3b-6fab-4355-9ad9-19b3fdc366a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[' LABEL'])\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822302db-e59f-4bad-b824-1f63419ac471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4\n",
       "1        0\n",
       "2        5\n",
       "3        2\n",
       "4        4\n",
       "        ..\n",
       "10060    5\n",
       "10061    6\n",
       "10062    4\n",
       "10063    2\n",
       "10064    0\n",
       "Name: label, Length: 10065, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf46c3b-f6c6-40a9-9f69-1700c67a80be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> anger\n",
      "1 -> fear\n",
      "2 -> joy\n",
      "3 -> love\n",
      "4 -> none\n",
      "5 -> sadness\n",
      "6 -> surprise\n",
      "7 -> sympathy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#We treated the none label as NEUTRAL/NO sentiment\n",
    "0 -> anger\n",
    "1 -> fear\n",
    "2 -> joy\n",
    "3 -> love\n",
    "4 -> neutral (none)\n",
    "5 -> sadness\n",
    "6 -> surprise\n",
    "7 -> sympathy\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labels = df[' LABEL']\n",
    "\n",
    "for code, label in enumerate(le.classes_):\n",
    "    print(f\"{code} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953895db-a7f7-4930-9366-2e3169a25976",
   "metadata": {},
   "source": [
    "# **PART TWO:**\n",
    "## Text Representations\n",
    "### **1) TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bb718e-4de0-4a01-9d3a-ee57bb8e6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75845dc8-da23-4d5d-89d8-6af51664a7f0",
   "metadata": {},
   "source": [
    "### **GridSearchCV for SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29a2ef9-689b-47fd-959b-6a83c9dc7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 1, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6275371105671033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "grid = GridSearchCV(SVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid.fit(X_tfidf_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best F1 Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8814cee3-ec88-4881-b912-407ce637eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8052, 28312)\n",
      "['Ø§Ø¨Ø§' 'Ø§Ø¨Ø§Ø¡' 'Ø§Ø¨Ø§Ø­' 'Ø§Ø¨Ø§Ø¯Ù‡' 'Ø§Ø¨Ø§Ø¸Ù‡' 'Ø§Ø¨Ø§Ù„Øº' 'Ø§Ø¨Ø§Ù„ÙŠ' 'Ø§Ø¨ØªØ¯Ø§Ø¡' 'Ø§Ø¨ØªØ¯Ø§Ø¡ÙŠ'\n",
      " 'Ø§Ø¨ØªØ¯Ø§ÙŠÙŠ' 'Ø§Ø¨ØªØ¯ÙˆØ§' 'Ø§Ø¨ØªØ¯ÙŠ' 'Ø§Ø¨ØªØ¯ÙŠØª' 'Ø§Ø¨ØªØ¯ÙŠØªÙˆØ§' 'Ø§Ø¨ØªØ¯ÙŠÙ†Ø§' 'Ø§Ø¨ØªØ²Ø§Ø²'\n",
      " 'Ø§Ø¨ØªØ³Ø§Ù…' 'Ø§Ø¨ØªØ³Ø§Ù…ØªÙƒ' 'Ø§Ø¨ØªØ³Ø§Ù…Ù‡' 'Ø§Ø¨ØªØ³Ø³Ù…' 'Ø§Ø¨ØªØ³Ù…' 'Ø§Ø¨ØªØ³Ù…Øª' 'Ø§Ø¨ØªØ³Ù…Ù†Ø§'\n",
      " 'Ø§Ø¨ØªØ³Ù…ÙŠ' 'Ø§Ø¨ØªØ¹Ø¯ÙŠ' 'Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª' 'Ø§Ø¨ØªÙ„Ø§Ø¡' 'Ø§Ø¨ØªÙ„Ø§Ù†Ø§' 'Ø§Ø¨ØªÙ‡Ø§Ø¬' 'Ø§Ø¨ØªÙ‡Ø§Ø¬Ø§']\n"
     ]
    }
   ],
   "source": [
    "'''print(X_tfidf_train.toarray())  # Print training TF-IDF matrix\n",
    "print(X_tfidf_test.toarray())   # Print test TF-IDF matrix\n",
    "'''\n",
    "\n",
    "print(X_tfidf_train.shape)  \n",
    "print(tfidf.get_feature_names_out()[20:50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fce4b-e712-4702-b76c-44acbac595af",
   "metadata": {},
   "source": [
    "### **2) Word2Vec (Using AraVec)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d6953e-348a-4f4e-8df1-b9ec8c199a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile('tweet_cbow_300.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('tweet_cbow_300')\n",
    "\n",
    "model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "def get_w2v_avg(texts, model, dim=300):\n",
    "    import numpy as np\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        if vecs:\n",
    "            vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_w2v_train = get_w2v_avg(X_train, model)\n",
    "X_w2v_test = get_w2v_avg(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ca5928-bfcb-4ed4-9ff8-ed4a13c3d7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.35434431e-01, -1.51483759e-01,  3.37284058e-01, -3.55382979e-01,\n",
       "       -4.92361069e-01, -2.73032784e-01,  1.21263318e-01, -3.40679824e-01,\n",
       "       -6.83230311e-02, -1.00465231e-01, -5.51082909e-01, -8.98964144e-03,\n",
       "       -1.08159799e-02, -4.55033660e-01, -8.59628201e-01,  4.73581962e-02,\n",
       "        6.12171181e-02, -2.88522780e-01, -1.80102289e-01, -1.89193189e-01,\n",
       "       -3.72576155e-02,  5.61842263e-01,  1.00205541e+00,  4.72833127e-01,\n",
       "       -2.90700436e-01,  2.40467682e-01,  3.10135603e-01,  3.69511023e-02,\n",
       "       -1.52749002e-01, -2.69441158e-01,  1.42771110e-01,  3.89136195e-01,\n",
       "        2.73712358e-04,  1.88248217e-01, -1.22218125e-01, -1.39235482e-01,\n",
       "       -4.63970929e-01, -1.37733489e-01, -4.95946817e-02,  2.15869486e-01,\n",
       "        4.50125962e-01, -3.48630105e-03, -1.53986916e-01,  2.13411510e-01,\n",
       "       -8.41119215e-02,  2.67761022e-01,  6.97255507e-02,  1.48830786e-01,\n",
       "        1.70076355e-01, -4.53415126e-01, -3.15578997e-01, -3.15471798e-01,\n",
       "       -1.07801192e-01, -2.49377936e-01,  1.17301494e-01,  2.68498242e-01,\n",
       "        3.95528153e-02,  2.68919989e-02, -4.79750112e-02, -9.56445113e-02,\n",
       "        2.58214861e-01,  2.86779344e-01, -2.24055246e-01,  2.14069366e-01,\n",
       "        2.87577718e-01, -6.34880245e-01, -3.01945180e-01,  1.48631074e-03,\n",
       "       -2.10053369e-01, -1.66314259e-01,  2.93708950e-01,  1.43018603e-01,\n",
       "       -3.20981778e-02, -2.25170568e-01,  1.26706958e-01, -4.16878909e-01,\n",
       "        6.83819726e-02, -1.01826660e-01, -3.67194153e-02,  7.74077773e-01,\n",
       "       -3.29086959e-01,  4.04548764e-01,  1.70900062e-01, -7.11730838e-01,\n",
       "       -1.91778503e-03,  1.52219281e-01, -1.99370831e-03, -3.30326818e-02,\n",
       "       -5.87312579e-01, -2.93068588e-01,  8.28525484e-01, -1.33406624e-01,\n",
       "       -4.36790049e-01,  9.67970490e-02,  1.09481119e-01,  7.92078450e-02,\n",
       "        2.78701991e-01,  2.95192301e-01, -2.06174612e-01,  3.13034445e-01,\n",
       "        2.94859558e-01,  6.74948469e-02,  2.23672658e-01, -2.92562038e-01,\n",
       "        3.84890556e-01, -9.13019478e-02, -3.34544063e-01,  3.74895692e-01,\n",
       "        1.79880649e-01,  2.24618331e-01, -2.08907902e-01,  3.53651494e-01,\n",
       "       -6.03246033e-01, -4.82773818e-02,  1.69601887e-01, -3.72824937e-01,\n",
       "        2.74413139e-01,  5.32142878e-01, -1.65895790e-01, -2.17533812e-01,\n",
       "        1.04381228e-02, -1.30522288e-02,  2.06754610e-01, -2.75893182e-01,\n",
       "        8.50044861e-02, -1.06326431e-01, -1.88232422e-01,  2.59238362e-01,\n",
       "       -2.65616179e-01, -2.44311497e-01,  6.79970384e-01, -1.26797846e-02,\n",
       "        3.17573361e-02, -1.83140606e-01,  7.31933638e-02,  3.18250060e-01,\n",
       "       -6.35975972e-02,  4.32931818e-02,  7.84825087e-02, -1.60330087e-01,\n",
       "        5.47827959e-01,  3.98161292e-01, -8.23387969e-03,  1.18066318e-01,\n",
       "       -5.49207032e-01, -4.18841809e-01, -1.68019217e-02,  2.55628079e-01,\n",
       "       -4.86007333e-01, -2.36295760e-01,  3.07534099e-01, -4.75167543e-01,\n",
       "       -4.94762957e-01,  7.02560246e-02, -3.12993154e-02,  5.35957105e-02,\n",
       "       -2.93929368e-01, -1.27793103e-02, -4.23575819e-01, -1.09686382e-01,\n",
       "        4.09446210e-01, -3.78513753e-01,  5.04258275e-02,  7.20496550e-02,\n",
       "       -4.06238228e-01, -3.55408013e-01,  5.43650761e-02, -1.83767781e-01,\n",
       "        1.69176489e-01,  5.42909205e-01, -6.24470823e-02,  3.85818005e-01,\n",
       "        5.49543679e-01, -5.09104013e-01, -8.06852728e-02,  2.68318713e-01,\n",
       "        5.40568948e-01,  2.24595994e-01, -2.54202634e-01,  2.83886373e-01,\n",
       "        1.26590747e-02,  3.52314524e-02, -5.81344701e-02, -1.99785650e-01,\n",
       "        1.75081551e-01, -2.14452684e-01, -1.20070994e-01, -1.82369366e-01,\n",
       "       -1.13933794e-02,  3.67727399e-01,  3.00231099e-01, -3.02584797e-01,\n",
       "        9.03136209e-02,  1.52182654e-01, -7.00612068e-02,  2.00972930e-01,\n",
       "       -7.52979696e-01,  8.33130702e-02, -5.58676541e-01,  1.69457927e-01,\n",
       "        1.41362473e-01,  3.74291763e-02,  3.32962364e-01, -3.09124291e-01,\n",
       "       -1.06919475e-01,  5.58121726e-02,  1.91431902e-02,  2.47035936e-01,\n",
       "       -9.31564048e-02, -2.16896698e-01,  1.57634825e-01, -1.83086112e-01,\n",
       "       -1.92761913e-01, -7.77406931e-01, -7.43172467e-01,  5.76056302e-01,\n",
       "        3.65952909e-01, -4.11491334e-01,  4.42354381e-01,  3.56680453e-01,\n",
       "        8.28003883e-02,  2.36868948e-01, -1.37137413e-01,  1.05751693e-01,\n",
       "       -8.86296034e-02,  1.08101904e+00,  2.71077338e-03, -5.77580370e-02,\n",
       "        3.57857347e-01, -4.04748060e-02, -4.21256453e-01, -7.59374738e-01,\n",
       "       -7.08343148e-01,  1.22344986e-01,  4.71874386e-01,  7.03071356e-01,\n",
       "        5.33676520e-02,  8.52137282e-02, -2.90988058e-01, -3.66999418e-01,\n",
       "       -3.13192457e-01, -4.20873106e-01, -2.20452473e-01,  1.84943289e-01,\n",
       "       -1.64246440e-01,  7.68142045e-02,  3.21033210e-01,  1.64172322e-01,\n",
       "       -7.75052905e-02, -1.83549047e-01,  2.20004201e-01,  6.44767880e-01,\n",
       "       -3.34278643e-01,  8.57967079e-01, -4.24266933e-03,  2.20335588e-01,\n",
       "       -4.77467000e-01, -4.76665169e-01,  1.74962237e-01,  4.41286862e-01,\n",
       "       -1.20406069e-01, -2.16094226e-01, -5.46302013e-02, -7.35873431e-02,\n",
       "       -3.08366954e-01, -2.16241419e-01,  1.40388340e-01,  3.54675680e-01,\n",
       "       -2.96950698e-01,  5.68401925e-02,  2.91952342e-01, -9.38755125e-02,\n",
       "       -1.10020280e-01, -1.51138201e-01, -1.00478992e-01,  1.00410628e+00,\n",
       "       -8.96095932e-02, -4.09820139e-01,  6.54600635e-02, -1.08809769e-01,\n",
       "        1.68196768e-01,  4.25695360e-01,  3.40904891e-01, -5.85984290e-01,\n",
       "       -7.60540590e-02, -6.31432002e-03, -1.21913202e-01, -8.08710232e-02,\n",
       "        1.61575288e-01, -7.76484236e-02,  2.68524319e-01, -5.13226986e-01,\n",
       "       -1.68808158e-02,  1.24973081e-01, -4.56554443e-01, -4.16365594e-01,\n",
       "       -1.28806248e-01, -3.86222363e-01, -3.01771611e-02,  1.75069705e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93449b2-ef9a-445b-b519-4bb3a714e763",
   "metadata": {},
   "source": [
    "### **3) BoW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e83b2f-4778-44bf-b0ac-c9579284673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "X_bow_train = bow.fit_transform(X_train)\n",
    "X_bow_test = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e6f0e34-e3ba-43dc-ac14-b1903873cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ù„Ù‡: 1\n",
      "Ø¯ÙˆÙ„: 1\n",
      "Ø§Ù„Ù†Ø¸Ø§Ù…: 2\n",
      "Ø³ÙˆØ±ÙŠØ§: 1\n",
      "ÙØ§Ù„Ø¨Ø¯Ø§ÙŠÙ‡: 1\n",
      "Ø´Ø¹Ø¨: 3\n",
      "ÙŠØ±ÙŠØ¯: 2\n",
      "Ø§ØµÙ„Ø§Ø­: 1\n",
      "Ø§Ø³Ù‚Ø§Ø·: 1\n",
      "ÙŠØ¯ÙØ¹: 1\n",
      "Ø«Ù…Ù†: 1\n",
      "ØµØ±Ø§Ø¹Ø§Øª: 1\n",
      "ÙˆØ§Ù‚ÙˆØ§Ù…: 1\n",
      "Ø¹ÙˆÙ†Ù‡Ù…: 1\n"
     ]
    }
   ],
   "source": [
    "# Total frequency of each word across all tweets (corpus-wide)\n",
    "\n",
    "row = 84\n",
    "\n",
    "nonzero_indices = X_bow_train[row].nonzero()[1]\n",
    "\n",
    "features = bow.get_feature_names_out()\n",
    "counts = X_bow_train[row, nonzero_indices].toarray().flatten()\n",
    "\n",
    "for feat, count in zip(features[nonzero_indices], counts):\n",
    "    print(f\"{feat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7edad6-110d-4386-a3f0-7b729f0bc6c9",
   "metadata": {},
   "source": [
    "# **PART THREE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c7b80a-72aa-461c-ae6c-4f60f891f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.66      0.63       276\n",
      "        fear       0.78      0.89      0.83       259\n",
      "         joy       0.59      0.29      0.39       268\n",
      "        love       0.66      0.67      0.67       250\n",
      "        none       0.42      0.94      0.58       307\n",
      "     sadness       0.62      0.22      0.33       258\n",
      "    surprise       0.85      0.17      0.28       201\n",
      "    sympathy       0.81      0.82      0.81       194\n",
      "\n",
      "    accuracy                           0.60      2013\n",
      "   macro avg       0.67      0.58      0.57      2013\n",
      "weighted avg       0.65      0.60      0.57      2013\n",
      "\n",
      "\n",
      "SVM (Linear Kernel, GridSearchCV) with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.71      0.60       276\n",
      "        fear       0.99      0.88      0.93       259\n",
      "         joy       0.53      0.40      0.46       268\n",
      "        love       0.66      0.61      0.63       250\n",
      "        none       0.60      0.93      0.73       307\n",
      "     sadness       0.48      0.35      0.40       258\n",
      "    surprise       0.69      0.40      0.51       201\n",
      "    sympathy       0.84      0.86      0.85       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.67      0.64      0.64      2013\n",
      "weighted avg       0.66      0.65      0.64      2013\n",
      "\n",
      "\n",
      "Decision Tree with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.45      0.44       276\n",
      "        fear       0.97      0.87      0.92       259\n",
      "         joy       0.32      0.31      0.31       268\n",
      "        love       0.61      0.48      0.54       250\n",
      "        none       0.63      0.69      0.66       307\n",
      "     sadness       0.28      0.38      0.32       258\n",
      "    surprise       0.45      0.40      0.42       201\n",
      "    sympathy       0.77      0.73      0.75       194\n",
      "\n",
      "    accuracy                           0.54      2013\n",
      "   macro avg       0.56      0.54      0.55      2013\n",
      "weighted avg       0.56      0.54      0.54      2013\n",
      "\n",
      "\n",
      "Random Forest with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.54      0.56       276\n",
      "        fear       0.94      0.91      0.93       259\n",
      "         joy       0.43      0.31      0.36       268\n",
      "        love       0.70      0.53      0.60       250\n",
      "        none       0.58      0.93      0.71       307\n",
      "     sadness       0.36      0.38      0.37       258\n",
      "    surprise       0.59      0.42      0.49       201\n",
      "    sympathy       0.77      0.87      0.81       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.62      0.61      0.60      2013\n",
      "weighted avg       0.61      0.61      0.60      2013\n",
      "\n",
      "\n",
      "AdaBoost with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.22      0.90      0.35       276\n",
      "        fear       0.99      0.71      0.83       259\n",
      "         joy       0.00      0.00      0.00       268\n",
      "        love       0.94      0.19      0.31       250\n",
      "        none       0.56      0.97      0.71       307\n",
      "     sadness       0.05      0.00      0.01       258\n",
      "    surprise       0.00      0.00      0.00       201\n",
      "    sympathy       0.85      0.35      0.50       194\n",
      "\n",
      "    accuracy                           0.42      2013\n",
      "   macro avg       0.45      0.39      0.34      2013\n",
      "weighted avg       0.45      0.42      0.35      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'), # rbf: Radial Basis Function Kernal / Gaussian Kernal (not linearly separable)\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with TF-IDF\")\n",
    "    model.fit(X_tfidf_train, y_train)\n",
    "    y_pred = model.predict(X_tfidf_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#SVM Based on the report\n",
    "'''\n",
    "# Using Grid-Search tuning\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (Linear Kernel, GridSearchCV)\": SVC(C=1, kernel='linear'), #C is the regularization parameter. \n",
    "                                                                    #It controls how much you want to avoid misclassifying each training example.\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with TF-IDF\")\n",
    "    model.fit(X_tfidf_train, y_train)\n",
    "    y_pred = model.predict(X_tfidf_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e1fb73-ed6a-4332-9b11-5d9d9177f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5134    0.6268    0.5644       276\n",
      "           1     0.6420    0.6371    0.6395       259\n",
      "           2     0.4267    0.2388    0.3062       268\n",
      "           3     0.5443    0.6880    0.6078       250\n",
      "           4     0.4562    0.7459    0.5661       307\n",
      "           5     0.2418    0.0853    0.1261       258\n",
      "           6     0.3270    0.2587    0.2889       201\n",
      "           7     0.6119    0.6340    0.6228       194\n",
      "\n",
      "    accuracy                         0.4968      2013\n",
      "   macro avg     0.4704    0.4893    0.4652      2013\n",
      "weighted avg     0.4696    0.4968    0.4673      2013\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print confusion matrix\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfusion_matrix\u001b[49m(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_w2v_train, y_train)\n",
    "y_pred = gnb.predict(X_w2v_test)\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123ca028-4e6e-4c7d-b952-2fba87e4b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.71      0.67       276\n",
      "        fear       0.89      0.88      0.88       259\n",
      "         joy       0.55      0.49      0.52       268\n",
      "        love       0.68      0.64      0.66       250\n",
      "        none       0.62      0.72      0.66       307\n",
      "     sadness       0.50      0.45      0.47       258\n",
      "    surprise       0.44      0.42      0.43       201\n",
      "    sympathy       0.81      0.78      0.80       194\n",
      "\n",
      "    accuracy                           0.64      2013\n",
      "   macro avg       0.64      0.64      0.64      2013\n",
      "weighted avg       0.64      0.64      0.64      2013\n",
      "\n",
      "\n",
      "SVM (RBF Kernel) with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.74      0.68       276\n",
      "        fear       0.95      0.82      0.88       259\n",
      "         joy       0.53      0.52      0.52       268\n",
      "        love       0.75      0.68      0.71       250\n",
      "        none       0.59      0.80      0.68       307\n",
      "     sadness       0.52      0.42      0.47       258\n",
      "    surprise       0.48      0.36      0.41       201\n",
      "    sympathy       0.81      0.79      0.80       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.66      0.64      0.64      2013\n",
      "weighted avg       0.65      0.65      0.65      2013\n",
      "\n",
      "\n",
      "Decision Tree with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.36      0.41      0.39       276\n",
      "        fear       0.53      0.49      0.51       259\n",
      "         joy       0.32      0.29      0.30       268\n",
      "        love       0.47      0.48      0.47       250\n",
      "        none       0.38      0.40      0.39       307\n",
      "     sadness       0.25      0.24      0.25       258\n",
      "    surprise       0.16      0.14      0.15       201\n",
      "    sympathy       0.42      0.44      0.43       194\n",
      "\n",
      "    accuracy                           0.37      2013\n",
      "   macro avg       0.36      0.36      0.36      2013\n",
      "weighted avg       0.36      0.37      0.37      2013\n",
      "\n",
      "\n",
      "Random Forest with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.69      0.58       276\n",
      "        fear       0.68      0.71      0.69       259\n",
      "         joy       0.48      0.33      0.39       268\n",
      "        love       0.59      0.72      0.65       250\n",
      "        none       0.51      0.73      0.60       307\n",
      "     sadness       0.41      0.21      0.28       258\n",
      "    surprise       0.44      0.24      0.31       201\n",
      "    sympathy       0.71      0.68      0.69       194\n",
      "\n",
      "    accuracy                           0.55      2013\n",
      "   macro avg       0.54      0.54      0.52      2013\n",
      "weighted avg       0.53      0.55      0.53      2013\n",
      "\n",
      "\n",
      "AdaBoost with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.46      0.51      0.48       276\n",
      "        fear       0.72      0.58      0.64       259\n",
      "         joy       0.40      0.24      0.30       268\n",
      "        love       0.50      0.61      0.55       250\n",
      "        none       0.44      0.53      0.48       307\n",
      "     sadness       0.22      0.24      0.23       258\n",
      "    surprise       0.19      0.21      0.20       201\n",
      "    sympathy       0.61      0.54      0.57       194\n",
      "\n",
      "    accuracy                           0.44      2013\n",
      "   macro avg       0.44      0.43      0.43      2013\n",
      "weighted avg       0.45      0.44      0.44      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000), # We tried iter [1000-5000] but still the same result\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with Word2Vec\")\n",
    "    model.fit(X_w2v_train, y_train)\n",
    "    y_pred = model.predict(X_w2v_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#Random Forest based on the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0324209b-b584-4e44-b5a5-69552db3695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.64      0.67      0.65       276\n",
      "        fear       0.78      0.90      0.83       259\n",
      "         joy       0.60      0.30      0.40       268\n",
      "        love       0.66      0.70      0.68       250\n",
      "        none       0.47      0.91      0.62       307\n",
      "     sadness       0.51      0.26      0.34       258\n",
      "    surprise       0.71      0.26      0.38       201\n",
      "    sympathy       0.73      0.87      0.80       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.64      0.61      0.59      2013\n",
      "weighted avg       0.63      0.62      0.59      2013\n",
      "\n",
      "\n",
      "SVM (RBF Kernel) with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.60      0.58       276\n",
      "        fear       0.99      0.86      0.92       259\n",
      "         joy       0.36      0.50      0.42       268\n",
      "        love       0.77      0.51      0.61       250\n",
      "        none       0.57      0.96      0.72       307\n",
      "     sadness       0.51      0.24      0.32       258\n",
      "    surprise       0.60      0.32      0.42       201\n",
      "    sympathy       0.72      0.79      0.75       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.64      0.60      0.59      2013\n",
      "weighted avg       0.63      0.61      0.60      2013\n",
      "\n",
      "\n",
      "Decision Tree with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.52      0.52       276\n",
      "        fear       0.96      0.90      0.93       259\n",
      "         joy       0.33      0.28      0.30       268\n",
      "        love       0.64      0.48      0.55       250\n",
      "        none       0.62      0.73      0.67       307\n",
      "     sadness       0.31      0.44      0.37       258\n",
      "    surprise       0.43      0.38      0.40       201\n",
      "    sympathy       0.85      0.78      0.82       194\n",
      "\n",
      "    accuracy                           0.57      2013\n",
      "   macro avg       0.58      0.56      0.57      2013\n",
      "weighted avg       0.58      0.57      0.57      2013\n",
      "\n",
      "\n",
      "Random Forest with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.52      0.56       276\n",
      "        fear       0.96      0.91      0.93       259\n",
      "         joy       0.38      0.37      0.37       268\n",
      "        love       0.69      0.55      0.61       250\n",
      "        none       0.59      0.92      0.72       307\n",
      "     sadness       0.37      0.39      0.38       258\n",
      "    surprise       0.58      0.38      0.46       201\n",
      "    sympathy       0.87      0.86      0.86       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.63      0.61      0.61      2013\n",
      "weighted avg       0.62      0.62      0.61      2013\n",
      "\n",
      "\n",
      "AdaBoost with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.22      0.90      0.35       276\n",
      "        fear       0.99      0.71      0.83       259\n",
      "         joy       0.00      0.00      0.00       268\n",
      "        love       0.94      0.19      0.31       250\n",
      "        none       0.56      0.97      0.71       307\n",
      "     sadness       0.00      0.00      0.00       258\n",
      "    surprise       0.00      0.00      0.00       201\n",
      "    sympathy       0.85      0.35      0.50       194\n",
      "\n",
      "    accuracy                           0.42      2013\n",
      "   macro avg       0.45      0.39      0.34      2013\n",
      "weighted avg       0.44      0.42      0.35      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with BoW\")\n",
    "    model.fit(X_bow_train, y_train)\n",
    "    y_pred = model.predict(X_bow_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#Random Forest based on the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73591232-72ad-4919-9a16-0334cdb56944",
   "metadata": {},
   "source": [
    "# **PART FOUR:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b18254-04f9-4cff-9a17-f8e2527c170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "227/227 [==============================] - 7s 12ms/step - loss: 1.6033 - accuracy: 0.4256 - val_loss: 1.1958 - val_accuracy: 0.6042\n",
      "Epoch 2/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.2513 - accuracy: 0.5610 - val_loss: 1.0598 - val_accuracy: 0.6377\n",
      "Epoch 3/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.1388 - accuracy: 0.6079 - val_loss: 1.0268 - val_accuracy: 0.6514\n",
      "Epoch 4/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.0768 - accuracy: 0.6289 - val_loss: 0.9980 - val_accuracy: 0.6625\n",
      "Epoch 5/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.0137 - accuracy: 0.6433 - val_loss: 0.9876 - val_accuracy: 0.6600\n",
      "Epoch 6/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.9675 - accuracy: 0.6686 - val_loss: 1.0072 - val_accuracy: 0.6563\n",
      "Epoch 7/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.9214 - accuracy: 0.6755 - val_loss: 0.9706 - val_accuracy: 0.6787\n",
      "Epoch 8/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8953 - accuracy: 0.6842 - val_loss: 1.0171 - val_accuracy: 0.6625\n",
      "Epoch 9/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8597 - accuracy: 0.7001 - val_loss: 0.9657 - val_accuracy: 0.6787\n",
      "Epoch 10/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8416 - accuracy: 0.7083 - val_loss: 0.9866 - val_accuracy: 0.6849\n",
      "Epoch 11/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8076 - accuracy: 0.7161 - val_loss: 0.9873 - val_accuracy: 0.6712\n",
      "Epoch 12/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.7908 - accuracy: 0.7160 - val_loss: 1.0052 - val_accuracy: 0.6638\n",
      "63/63 [==============================] - 1s 4ms/step\n",
      "\n",
      "Feed-Forward NN (Keras) Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       276\n",
      "           1       0.93      0.88      0.91       259\n",
      "           2       0.57      0.45      0.50       268\n",
      "           3       0.73      0.69      0.71       250\n",
      "           4       0.56      0.82      0.66       307\n",
      "           5       0.56      0.45      0.50       258\n",
      "           6       0.48      0.41      0.44       201\n",
      "           7       0.82      0.82      0.82       194\n",
      "\n",
      "    accuracy                           0.66      2013\n",
      "   macro avg       0.66      0.65      0.65      2013\n",
      "weighted avg       0.66      0.66      0.65      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode the labels for multi-class output\n",
    "lb = LabelBinarizer()  # y_train contains labels\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Use Word2Vec averaged vectors as input\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(300,)))  # 300 = Word2Vec vector size\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))      # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_w2v_train, y_train_oh, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=le.classes_))'''\n",
    "\n",
    "##_____________________________________________________________________________________________________________________________##\n",
    "\n",
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode the labels for multi-class output\n",
    "lb = LabelBinarizer()\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Define the model with multiple hidden layers and dropout for regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='ReLU', input_shape=(300,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='ReLU'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # default learning rate ~0.001\n",
    "\n",
    "model.fit(X_w2v_train, y_train_oh, epochs=15, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=le.classes_))'''\n",
    "\n",
    "##_____________________________________________________________________________________________________________________________##\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Optional: compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(300,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    X_w2v_train, y_train_oh,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=[str(cls) for cls in lb.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013830e-0fce-45ee-8a0f-f2a1b941816b",
   "metadata": {},
   "source": [
    "# **PART FIVE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3754ab9-2e29-4f79-b9d3-95858be0c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "202/202 [==============================] - 165s 526ms/step - loss: 1.3847 - accuracy: 0.5204 - val_loss: 1.1010 - val_accuracy: 0.6195\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 94s 467ms/step - loss: 0.9810 - accuracy: 0.6614 - val_loss: 1.0280 - val_accuracy: 0.6505\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 95s 468ms/step - loss: 0.8267 - accuracy: 0.7170 - val_loss: 1.0185 - val_accuracy: 0.6530\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 95s 469ms/step - loss: 0.7245 - accuracy: 0.7552 - val_loss: 1.0138 - val_accuracy: 0.6611\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 95s 472ms/step - loss: 0.6308 - accuracy: 0.7844 - val_loss: 1.0341 - val_accuracy: 0.6536\n",
      "63/63 [==============================] - 12s 168ms/step\n",
      "\n",
      "LSTM-Improved Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67       276\n",
      "           1       0.96      0.92      0.94       259\n",
      "           2       0.51      0.47      0.49       268\n",
      "           3       0.71      0.65      0.68       250\n",
      "           4       0.65      0.77      0.71       307\n",
      "           5       0.44      0.41      0.42       258\n",
      "           6       0.47      0.35      0.40       201\n",
      "           7       0.85      0.85      0.85       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.65      0.65      0.65      2013\n",
      "weighted avg       0.65      0.65      0.65      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "max_seq_len = 100\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_seq_len)\n",
    "\n",
    "# 2. Prepare embedding matrix from Word2Vec\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(5000, len(word_index) + 1)\n",
    "embedding_dim = w2v_model.vector_size  # usually 300\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# 3. One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_seq_len,\n",
    "    trainable=False  # freeze embeddings To preserve the original semantic information from high-quality \n",
    "                    # pre-trained embeddings like Word2Vec, GloVe, or AraVec & reduce training time \n",
    "                    # and prevent overfitting, especially with small datasets.\n",
    "))\n",
    "\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# 6. Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 7. Callbacks for early stopping and saving best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# 8. Train\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# 9. Predict & evaluate\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in lb.classes_]))\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Load your pre-trained Word2Vec model\n",
    "w2v_model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "# 2. Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "max_seq_len = 100\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_seq_len)\n",
    "\n",
    "# 3. Prepare embedding matrix using w2v_model\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(5000, len(word_index) + 1)\n",
    "embedding_dim = w2v_model.vector_size  # 300 for your CBOW model\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# 4. One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# 5. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_seq_len,\n",
    "    trainable=False \n",
    "))\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# 7. Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# 9. Train model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# 10. Predict and evaluate\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\nLSTM-Improved Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in lb.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f0d42-0256-469c-92fa-7a514be78b10",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "715f05a4-aa68-4ecd-a0e9-35371fb908ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "101/101 [==============================] - 275s 2s/step - loss: 1.6226 - accuracy: 0.4248 - val_loss: 1.2487 - val_accuracy: 0.5829\n",
      "Epoch 2/5\n",
      "101/101 [==============================] - 175s 2s/step - loss: 1.1420 - accuracy: 0.6103 - val_loss: 1.0591 - val_accuracy: 0.6363\n",
      "Epoch 3/5\n",
      "101/101 [==============================] - 170s 2s/step - loss: 0.9841 - accuracy: 0.6636 - val_loss: 1.0375 - val_accuracy: 0.6555\n",
      "Epoch 4/5\n",
      "101/101 [==============================] - 169s 2s/step - loss: 0.8489 - accuracy: 0.7014 - val_loss: 1.0244 - val_accuracy: 0.6524\n",
      "Epoch 5/5\n",
      "101/101 [==============================] - 172s 2s/step - loss: 0.7666 - accuracy: 0.7339 - val_loss: 1.0616 - val_accuracy: 0.6605\n",
      "63/63 [==============================] - 16s 212ms/step\n",
      "\n",
      "BiLSTM Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67       276\n",
      "           1       0.94      0.89      0.92       259\n",
      "           2       0.53      0.44      0.48       268\n",
      "           3       0.70      0.66      0.68       250\n",
      "           4       0.59      0.85      0.70       307\n",
      "           5       0.50      0.40      0.45       258\n",
      "           6       0.50      0.35      0.41       201\n",
      "           7       0.84      0.85      0.84       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.65      0.65      0.64      2013\n",
      "weighted avg       0.65      0.65      0.64      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import zipfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load Pre-trained Word2Vec Model\n",
    "with zipfile.ZipFile('tweet_cbow_300.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('tweet_cbow_300')\n",
    "\n",
    "w2v_model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "embedding_dim = 300\n",
    "\n",
    "# Tokenize and Pad Sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=150)\n",
    "\n",
    "# Build Embedding Matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(10000, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Encode Labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_pad, X_test_pad, y_train_oh, y_test_oh = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BiLSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=150,\n",
    "    trainable=False  \n",
    "))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training with Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_bilstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad, y_train_oh,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test_pad).argmax(axis=1)\n",
    "y_true = y_test_oh.argmax(axis=1)\n",
    "\n",
    "print(\"\\nBiLSTM Model Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(cls) for cls in lb.classes_]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284be3d-4ec2-4f3c-99d1-2a41df718a5d",
   "metadata": {},
   "source": [
    "# **AraBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8aaf020-d2bf-4fbf-aa99-2300c23ec7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     81\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3fx4ebxh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1630\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1630\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(set(y))  # Replace `y` with your label list\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Tokenization\n",
    "def convert_examples(texts, labels):\n",
    "    return [\n",
    "        InputExample(guid=str(i), text_a=text, label=label)\n",
    "        for i, (text, label) in enumerate(zip(texts, labels))\n",
    "    ]\n",
    "\n",
    "def convert_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "\n",
    "    for e in examples:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        attention_mask = inputs[\"attention_mask\"][0]\n",
    "        features.append(({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }, e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield f\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "train_examples = convert_examples(X_train, y_train)\n",
    "test_examples = convert_examples(X_test, y_test)\n",
    "\n",
    "train_dataset = convert_to_tf_dataset(train_examples, tokenizer).shuffle(100).batch(16)\n",
    "test_dataset = convert_to_tf_dataset(test_examples, tokenizer).batch(16)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_probs = model.predict(test_dataset).logits\n",
    "y_pred = tf.argmax(y_pred_probs, axis=1).numpy()\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266ec8b-86be-4269-a9fe-596b31cfad30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
