{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17628cd-dd81-4897-b727-76f8fa64a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7c4958-a261-4479-8ca8-39be7dc528ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>الاوليمبياد الجايه هكون لسه ف الكليه ..</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>كتنا نيله ف حظنا الهباب xD</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>10061</td>\n",
       "      <td>2222: يلا يا جماعه حفله عمرو دياب خلصت نريح شو...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>10062</td>\n",
       "      <td>Mohamed5: اييييه دااا 😲😲 اوزيييل❤</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>10063</td>\n",
       "      <td>عملتلها ريتويت بمناسبه ساره بتاعه الاوليمبياد 😃</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>10064</td>\n",
       "      <td>وعليك قبلنا يانجم النجوم ياعندليب الحب والاحساس</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>10065</td>\n",
       "      <td>AlHamad يطلع ننهم كل شي سيء ووضيع كل خساسه الع...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10065 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                              TWEET     LABEL\n",
       "0          1            الاوليمبياد الجايه هكون لسه ف الكليه ..      none\n",
       "1          2  عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...     anger\n",
       "2          3                         كتنا نيله ف حظنا الهباب xD   sadness\n",
       "3          4  جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...       joy\n",
       "4          5  الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...      none\n",
       "...      ...                                                ...       ...\n",
       "10060  10061  2222: يلا يا جماعه حفله عمرو دياب خلصت نريح شو...   sadness\n",
       "10061  10062                  Mohamed5: اييييه دااا 😲😲 اوزيييل❤  surprise\n",
       "10062  10063    عملتلها ريتويت بمناسبه ساره بتاعه الاوليمبياد 😃      none\n",
       "10063  10064    وعليك قبلنا يانجم النجوم ياعندليب الحب والاحساس       joy\n",
       "10064  10065  AlHamad يطلع ننهم كل شي سيء ووضيع كل خساسه الع...     anger\n",
       "\n",
       "[10065 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Emotional-Tone-Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ad2b8-a4bd-47d6-82d3-638f17aeb746",
   "metadata": {},
   "source": [
    "# **PART ONE:**\n",
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84aad15f-9b02-43e8-a15f-a89d024a324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "def normalize_repeated_letters(text):\n",
    "\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = normalize_repeated_letters(text)\n",
    "\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "\n",
    "    \n",
    "    text = strip_tashkeel(strip_tatweel(text))\n",
    "\n",
    "\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "    \n",
    "df['clean_text'] = df[' TWEET'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e3670a-3480-4c1b-9f7d-07ffce0fce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       الاوليمبياد الجايه هكون لسه الكليه\n",
       "1        عجز الموازنه وصل الناتج المحلي يعني لسه اقل نف...\n",
       "2                                    كتنا نيله حظنا الهباب\n",
       "3        جميعنا نريد تحقيق اهدافنا تونس تالقت حراسه المرمي\n",
       "4        الاوليمبياد نظامها مختلف ومواعيد المونديال مكا...\n",
       "                               ...                        \n",
       "10060    يلا جماعه حفله عمرو دياب خلصت نريح شويه ونبدا ...\n",
       "10061                                            ايه اوزيل\n",
       "10062        عملتلها ريتويت بمناسبه ساره بتاعه الاوليمبياد\n",
       "10063      وعليك قبلنا يانجم النجوم ياعندليب الحب والاحساس\n",
       "10064    يطلع ننهم سيء ووضيع خساسه العالم تجمعت الايران...\n",
       "Name: clean_text, Length: 10065, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3e70d-59d0-4d4d-bd24-7234babccc50",
   "metadata": {},
   "source": [
    "## Label Encoding and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdadc3b-6fab-4355-9ad9-19b3fdc366a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[' LABEL'])\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822302db-e59f-4bad-b824-1f63419ac471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4\n",
       "1        0\n",
       "2        5\n",
       "3        2\n",
       "4        4\n",
       "        ..\n",
       "10060    5\n",
       "10061    6\n",
       "10062    4\n",
       "10063    2\n",
       "10064    0\n",
       "Name: label, Length: 10065, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf46c3b-f6c6-40a9-9f69-1700c67a80be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> anger\n",
      "1 -> fear\n",
      "2 -> joy\n",
      "3 -> love\n",
      "4 -> none\n",
      "5 -> sadness\n",
      "6 -> surprise\n",
      "7 -> sympathy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#We treated the none label as NEUTRAL/NO sentiment\n",
    "0 -> anger\n",
    "1 -> fear\n",
    "2 -> joy\n",
    "3 -> love\n",
    "4 -> neutral (none)\n",
    "5 -> sadness\n",
    "6 -> surprise\n",
    "7 -> sympathy\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labels = df[' LABEL']\n",
    "\n",
    "for code, label in enumerate(le.classes_):\n",
    "    print(f\"{code} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953895db-a7f7-4930-9366-2e3169a25976",
   "metadata": {},
   "source": [
    "# **PART TWO:**\n",
    "## Text Representations\n",
    "### **1) TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bb718e-4de0-4a01-9d3a-ee57bb8e6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75845dc8-da23-4d5d-89d8-6af51664a7f0",
   "metadata": {},
   "source": [
    "### **GridSearchCV for SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29a2ef9-689b-47fd-959b-6a83c9dc7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 1, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6275371105671033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "grid = GridSearchCV(SVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid.fit(X_tfidf_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best F1 Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8814cee3-ec88-4881-b912-407ce637eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8052, 28312)\n",
      "['ابا' 'اباء' 'اباح' 'اباده' 'اباظه' 'ابالغ' 'ابالي' 'ابتداء' 'ابتداءي'\n",
      " 'ابتدايي' 'ابتدوا' 'ابتدي' 'ابتديت' 'ابتديتوا' 'ابتدينا' 'ابتزاز'\n",
      " 'ابتسام' 'ابتسامتك' 'ابتسامه' 'ابتسسم' 'ابتسم' 'ابتسمت' 'ابتسمنا'\n",
      " 'ابتسمي' 'ابتعدي' 'ابتكارات' 'ابتلاء' 'ابتلانا' 'ابتهاج' 'ابتهاجا']\n"
     ]
    }
   ],
   "source": [
    "'''print(X_tfidf_train.toarray())  # Print training TF-IDF matrix\n",
    "print(X_tfidf_test.toarray())   # Print test TF-IDF matrix\n",
    "'''\n",
    "\n",
    "print(X_tfidf_train.shape)  \n",
    "print(tfidf.get_feature_names_out()[20:50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fce4b-e712-4702-b76c-44acbac595af",
   "metadata": {},
   "source": [
    "### **2) Word2Vec (Using AraVec)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d6953e-348a-4f4e-8df1-b9ec8c199a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile('tweet_cbow_300.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('tweet_cbow_300')\n",
    "\n",
    "model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "def get_w2v_avg(texts, model, dim=300):\n",
    "    import numpy as np\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        if vecs:\n",
    "            vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_w2v_train = get_w2v_avg(X_train, model)\n",
    "X_w2v_test = get_w2v_avg(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ca5928-bfcb-4ed4-9ff8-ed4a13c3d7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.35434431e-01, -1.51483759e-01,  3.37284058e-01, -3.55382979e-01,\n",
       "       -4.92361069e-01, -2.73032784e-01,  1.21263318e-01, -3.40679824e-01,\n",
       "       -6.83230311e-02, -1.00465231e-01, -5.51082909e-01, -8.98964144e-03,\n",
       "       -1.08159799e-02, -4.55033660e-01, -8.59628201e-01,  4.73581962e-02,\n",
       "        6.12171181e-02, -2.88522780e-01, -1.80102289e-01, -1.89193189e-01,\n",
       "       -3.72576155e-02,  5.61842263e-01,  1.00205541e+00,  4.72833127e-01,\n",
       "       -2.90700436e-01,  2.40467682e-01,  3.10135603e-01,  3.69511023e-02,\n",
       "       -1.52749002e-01, -2.69441158e-01,  1.42771110e-01,  3.89136195e-01,\n",
       "        2.73712358e-04,  1.88248217e-01, -1.22218125e-01, -1.39235482e-01,\n",
       "       -4.63970929e-01, -1.37733489e-01, -4.95946817e-02,  2.15869486e-01,\n",
       "        4.50125962e-01, -3.48630105e-03, -1.53986916e-01,  2.13411510e-01,\n",
       "       -8.41119215e-02,  2.67761022e-01,  6.97255507e-02,  1.48830786e-01,\n",
       "        1.70076355e-01, -4.53415126e-01, -3.15578997e-01, -3.15471798e-01,\n",
       "       -1.07801192e-01, -2.49377936e-01,  1.17301494e-01,  2.68498242e-01,\n",
       "        3.95528153e-02,  2.68919989e-02, -4.79750112e-02, -9.56445113e-02,\n",
       "        2.58214861e-01,  2.86779344e-01, -2.24055246e-01,  2.14069366e-01,\n",
       "        2.87577718e-01, -6.34880245e-01, -3.01945180e-01,  1.48631074e-03,\n",
       "       -2.10053369e-01, -1.66314259e-01,  2.93708950e-01,  1.43018603e-01,\n",
       "       -3.20981778e-02, -2.25170568e-01,  1.26706958e-01, -4.16878909e-01,\n",
       "        6.83819726e-02, -1.01826660e-01, -3.67194153e-02,  7.74077773e-01,\n",
       "       -3.29086959e-01,  4.04548764e-01,  1.70900062e-01, -7.11730838e-01,\n",
       "       -1.91778503e-03,  1.52219281e-01, -1.99370831e-03, -3.30326818e-02,\n",
       "       -5.87312579e-01, -2.93068588e-01,  8.28525484e-01, -1.33406624e-01,\n",
       "       -4.36790049e-01,  9.67970490e-02,  1.09481119e-01,  7.92078450e-02,\n",
       "        2.78701991e-01,  2.95192301e-01, -2.06174612e-01,  3.13034445e-01,\n",
       "        2.94859558e-01,  6.74948469e-02,  2.23672658e-01, -2.92562038e-01,\n",
       "        3.84890556e-01, -9.13019478e-02, -3.34544063e-01,  3.74895692e-01,\n",
       "        1.79880649e-01,  2.24618331e-01, -2.08907902e-01,  3.53651494e-01,\n",
       "       -6.03246033e-01, -4.82773818e-02,  1.69601887e-01, -3.72824937e-01,\n",
       "        2.74413139e-01,  5.32142878e-01, -1.65895790e-01, -2.17533812e-01,\n",
       "        1.04381228e-02, -1.30522288e-02,  2.06754610e-01, -2.75893182e-01,\n",
       "        8.50044861e-02, -1.06326431e-01, -1.88232422e-01,  2.59238362e-01,\n",
       "       -2.65616179e-01, -2.44311497e-01,  6.79970384e-01, -1.26797846e-02,\n",
       "        3.17573361e-02, -1.83140606e-01,  7.31933638e-02,  3.18250060e-01,\n",
       "       -6.35975972e-02,  4.32931818e-02,  7.84825087e-02, -1.60330087e-01,\n",
       "        5.47827959e-01,  3.98161292e-01, -8.23387969e-03,  1.18066318e-01,\n",
       "       -5.49207032e-01, -4.18841809e-01, -1.68019217e-02,  2.55628079e-01,\n",
       "       -4.86007333e-01, -2.36295760e-01,  3.07534099e-01, -4.75167543e-01,\n",
       "       -4.94762957e-01,  7.02560246e-02, -3.12993154e-02,  5.35957105e-02,\n",
       "       -2.93929368e-01, -1.27793103e-02, -4.23575819e-01, -1.09686382e-01,\n",
       "        4.09446210e-01, -3.78513753e-01,  5.04258275e-02,  7.20496550e-02,\n",
       "       -4.06238228e-01, -3.55408013e-01,  5.43650761e-02, -1.83767781e-01,\n",
       "        1.69176489e-01,  5.42909205e-01, -6.24470823e-02,  3.85818005e-01,\n",
       "        5.49543679e-01, -5.09104013e-01, -8.06852728e-02,  2.68318713e-01,\n",
       "        5.40568948e-01,  2.24595994e-01, -2.54202634e-01,  2.83886373e-01,\n",
       "        1.26590747e-02,  3.52314524e-02, -5.81344701e-02, -1.99785650e-01,\n",
       "        1.75081551e-01, -2.14452684e-01, -1.20070994e-01, -1.82369366e-01,\n",
       "       -1.13933794e-02,  3.67727399e-01,  3.00231099e-01, -3.02584797e-01,\n",
       "        9.03136209e-02,  1.52182654e-01, -7.00612068e-02,  2.00972930e-01,\n",
       "       -7.52979696e-01,  8.33130702e-02, -5.58676541e-01,  1.69457927e-01,\n",
       "        1.41362473e-01,  3.74291763e-02,  3.32962364e-01, -3.09124291e-01,\n",
       "       -1.06919475e-01,  5.58121726e-02,  1.91431902e-02,  2.47035936e-01,\n",
       "       -9.31564048e-02, -2.16896698e-01,  1.57634825e-01, -1.83086112e-01,\n",
       "       -1.92761913e-01, -7.77406931e-01, -7.43172467e-01,  5.76056302e-01,\n",
       "        3.65952909e-01, -4.11491334e-01,  4.42354381e-01,  3.56680453e-01,\n",
       "        8.28003883e-02,  2.36868948e-01, -1.37137413e-01,  1.05751693e-01,\n",
       "       -8.86296034e-02,  1.08101904e+00,  2.71077338e-03, -5.77580370e-02,\n",
       "        3.57857347e-01, -4.04748060e-02, -4.21256453e-01, -7.59374738e-01,\n",
       "       -7.08343148e-01,  1.22344986e-01,  4.71874386e-01,  7.03071356e-01,\n",
       "        5.33676520e-02,  8.52137282e-02, -2.90988058e-01, -3.66999418e-01,\n",
       "       -3.13192457e-01, -4.20873106e-01, -2.20452473e-01,  1.84943289e-01,\n",
       "       -1.64246440e-01,  7.68142045e-02,  3.21033210e-01,  1.64172322e-01,\n",
       "       -7.75052905e-02, -1.83549047e-01,  2.20004201e-01,  6.44767880e-01,\n",
       "       -3.34278643e-01,  8.57967079e-01, -4.24266933e-03,  2.20335588e-01,\n",
       "       -4.77467000e-01, -4.76665169e-01,  1.74962237e-01,  4.41286862e-01,\n",
       "       -1.20406069e-01, -2.16094226e-01, -5.46302013e-02, -7.35873431e-02,\n",
       "       -3.08366954e-01, -2.16241419e-01,  1.40388340e-01,  3.54675680e-01,\n",
       "       -2.96950698e-01,  5.68401925e-02,  2.91952342e-01, -9.38755125e-02,\n",
       "       -1.10020280e-01, -1.51138201e-01, -1.00478992e-01,  1.00410628e+00,\n",
       "       -8.96095932e-02, -4.09820139e-01,  6.54600635e-02, -1.08809769e-01,\n",
       "        1.68196768e-01,  4.25695360e-01,  3.40904891e-01, -5.85984290e-01,\n",
       "       -7.60540590e-02, -6.31432002e-03, -1.21913202e-01, -8.08710232e-02,\n",
       "        1.61575288e-01, -7.76484236e-02,  2.68524319e-01, -5.13226986e-01,\n",
       "       -1.68808158e-02,  1.24973081e-01, -4.56554443e-01, -4.16365594e-01,\n",
       "       -1.28806248e-01, -3.86222363e-01, -3.01771611e-02,  1.75069705e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['الاوليمبياد']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93449b2-ef9a-445b-b519-4bb3a714e763",
   "metadata": {},
   "source": [
    "### **3) BoW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e83b2f-4778-44bf-b0ac-c9579284673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "X_bow_train = bow.fit_transform(X_train)\n",
    "X_bow_test = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e6f0e34-e3ba-43dc-ac14-b1903873cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الله: 1\n",
      "دول: 1\n",
      "النظام: 2\n",
      "سوريا: 1\n",
      "فالبدايه: 1\n",
      "شعب: 3\n",
      "يريد: 2\n",
      "اصلاح: 1\n",
      "اسقاط: 1\n",
      "يدفع: 1\n",
      "ثمن: 1\n",
      "صراعات: 1\n",
      "واقوام: 1\n",
      "عونهم: 1\n"
     ]
    }
   ],
   "source": [
    "# Total frequency of each word across all tweets (corpus-wide)\n",
    "\n",
    "row = 84\n",
    "\n",
    "nonzero_indices = X_bow_train[row].nonzero()[1]\n",
    "\n",
    "features = bow.get_feature_names_out()\n",
    "counts = X_bow_train[row, nonzero_indices].toarray().flatten()\n",
    "\n",
    "for feat, count in zip(features[nonzero_indices], counts):\n",
    "    print(f\"{feat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7edad6-110d-4386-a3f0-7b729f0bc6c9",
   "metadata": {},
   "source": [
    "# **PART THREE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c7b80a-72aa-461c-ae6c-4f60f891f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.66      0.63       276\n",
      "        fear       0.78      0.89      0.83       259\n",
      "         joy       0.59      0.29      0.39       268\n",
      "        love       0.66      0.67      0.67       250\n",
      "        none       0.42      0.94      0.58       307\n",
      "     sadness       0.62      0.22      0.33       258\n",
      "    surprise       0.85      0.17      0.28       201\n",
      "    sympathy       0.81      0.82      0.81       194\n",
      "\n",
      "    accuracy                           0.60      2013\n",
      "   macro avg       0.67      0.58      0.57      2013\n",
      "weighted avg       0.65      0.60      0.57      2013\n",
      "\n",
      "\n",
      "SVM (Linear Kernel, GridSearchCV) with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.71      0.60       276\n",
      "        fear       0.99      0.88      0.93       259\n",
      "         joy       0.53      0.40      0.46       268\n",
      "        love       0.66      0.61      0.63       250\n",
      "        none       0.60      0.93      0.73       307\n",
      "     sadness       0.48      0.35      0.40       258\n",
      "    surprise       0.69      0.40      0.51       201\n",
      "    sympathy       0.84      0.86      0.85       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.67      0.64      0.64      2013\n",
      "weighted avg       0.66      0.65      0.64      2013\n",
      "\n",
      "\n",
      "Decision Tree with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.45      0.44       276\n",
      "        fear       0.97      0.87      0.92       259\n",
      "         joy       0.32      0.31      0.31       268\n",
      "        love       0.61      0.48      0.54       250\n",
      "        none       0.63      0.69      0.66       307\n",
      "     sadness       0.28      0.38      0.32       258\n",
      "    surprise       0.45      0.40      0.42       201\n",
      "    sympathy       0.77      0.73      0.75       194\n",
      "\n",
      "    accuracy                           0.54      2013\n",
      "   macro avg       0.56      0.54      0.55      2013\n",
      "weighted avg       0.56      0.54      0.54      2013\n",
      "\n",
      "\n",
      "Random Forest with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.54      0.56       276\n",
      "        fear       0.94      0.91      0.93       259\n",
      "         joy       0.43      0.31      0.36       268\n",
      "        love       0.70      0.53      0.60       250\n",
      "        none       0.58      0.93      0.71       307\n",
      "     sadness       0.36      0.38      0.37       258\n",
      "    surprise       0.59      0.42      0.49       201\n",
      "    sympathy       0.77      0.87      0.81       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.62      0.61      0.60      2013\n",
      "weighted avg       0.61      0.61      0.60      2013\n",
      "\n",
      "\n",
      "AdaBoost with TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.22      0.90      0.35       276\n",
      "        fear       0.99      0.71      0.83       259\n",
      "         joy       0.00      0.00      0.00       268\n",
      "        love       0.94      0.19      0.31       250\n",
      "        none       0.56      0.97      0.71       307\n",
      "     sadness       0.05      0.00      0.01       258\n",
      "    surprise       0.00      0.00      0.00       201\n",
      "    sympathy       0.85      0.35      0.50       194\n",
      "\n",
      "    accuracy                           0.42      2013\n",
      "   macro avg       0.45      0.39      0.34      2013\n",
      "weighted avg       0.45      0.42      0.35      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'), # rbf: Radial Basis Function Kernal / Gaussian Kernal (not linearly separable)\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with TF-IDF\")\n",
    "    model.fit(X_tfidf_train, y_train)\n",
    "    y_pred = model.predict(X_tfidf_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#SVM Based on the report\n",
    "'''\n",
    "# Using Grid-Search tuning\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (Linear Kernel, GridSearchCV)\": SVC(C=1, kernel='linear'), #C is the regularization parameter. \n",
    "                                                                    #It controls how much you want to avoid misclassifying each training example.\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with TF-IDF\")\n",
    "    model.fit(X_tfidf_train, y_train)\n",
    "    y_pred = model.predict(X_tfidf_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e1fb73-ed6a-4332-9b11-5d9d9177f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5134    0.6268    0.5644       276\n",
      "           1     0.6420    0.6371    0.6395       259\n",
      "           2     0.4267    0.2388    0.3062       268\n",
      "           3     0.5443    0.6880    0.6078       250\n",
      "           4     0.4562    0.7459    0.5661       307\n",
      "           5     0.2418    0.0853    0.1261       258\n",
      "           6     0.3270    0.2587    0.2889       201\n",
      "           7     0.6119    0.6340    0.6228       194\n",
      "\n",
      "    accuracy                         0.4968      2013\n",
      "   macro avg     0.4704    0.4893    0.4652      2013\n",
      "weighted avg     0.4696    0.4968    0.4673      2013\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print confusion matrix\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfusion_matrix\u001b[49m(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_w2v_train, y_train)\n",
    "y_pred = gnb.predict(X_w2v_test)\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123ca028-4e6e-4c7d-b952-2fba87e4b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.71      0.67       276\n",
      "        fear       0.89      0.88      0.88       259\n",
      "         joy       0.55      0.49      0.52       268\n",
      "        love       0.68      0.64      0.66       250\n",
      "        none       0.62      0.72      0.66       307\n",
      "     sadness       0.50      0.45      0.47       258\n",
      "    surprise       0.44      0.42      0.43       201\n",
      "    sympathy       0.81      0.78      0.80       194\n",
      "\n",
      "    accuracy                           0.64      2013\n",
      "   macro avg       0.64      0.64      0.64      2013\n",
      "weighted avg       0.64      0.64      0.64      2013\n",
      "\n",
      "\n",
      "SVM (RBF Kernel) with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.74      0.68       276\n",
      "        fear       0.95      0.82      0.88       259\n",
      "         joy       0.53      0.52      0.52       268\n",
      "        love       0.75      0.68      0.71       250\n",
      "        none       0.59      0.80      0.68       307\n",
      "     sadness       0.52      0.42      0.47       258\n",
      "    surprise       0.48      0.36      0.41       201\n",
      "    sympathy       0.81      0.79      0.80       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.66      0.64      0.64      2013\n",
      "weighted avg       0.65      0.65      0.65      2013\n",
      "\n",
      "\n",
      "Decision Tree with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.36      0.41      0.39       276\n",
      "        fear       0.53      0.49      0.51       259\n",
      "         joy       0.32      0.29      0.30       268\n",
      "        love       0.47      0.48      0.47       250\n",
      "        none       0.38      0.40      0.39       307\n",
      "     sadness       0.25      0.24      0.25       258\n",
      "    surprise       0.16      0.14      0.15       201\n",
      "    sympathy       0.42      0.44      0.43       194\n",
      "\n",
      "    accuracy                           0.37      2013\n",
      "   macro avg       0.36      0.36      0.36      2013\n",
      "weighted avg       0.36      0.37      0.37      2013\n",
      "\n",
      "\n",
      "Random Forest with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.69      0.58       276\n",
      "        fear       0.68      0.71      0.69       259\n",
      "         joy       0.48      0.33      0.39       268\n",
      "        love       0.59      0.72      0.65       250\n",
      "        none       0.51      0.73      0.60       307\n",
      "     sadness       0.41      0.21      0.28       258\n",
      "    surprise       0.44      0.24      0.31       201\n",
      "    sympathy       0.71      0.68      0.69       194\n",
      "\n",
      "    accuracy                           0.55      2013\n",
      "   macro avg       0.54      0.54      0.52      2013\n",
      "weighted avg       0.53      0.55      0.53      2013\n",
      "\n",
      "\n",
      "AdaBoost with Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.46      0.51      0.48       276\n",
      "        fear       0.72      0.58      0.64       259\n",
      "         joy       0.40      0.24      0.30       268\n",
      "        love       0.50      0.61      0.55       250\n",
      "        none       0.44      0.53      0.48       307\n",
      "     sadness       0.22      0.24      0.23       258\n",
      "    surprise       0.19      0.21      0.20       201\n",
      "    sympathy       0.61      0.54      0.57       194\n",
      "\n",
      "    accuracy                           0.44      2013\n",
      "   macro avg       0.44      0.43      0.43      2013\n",
      "weighted avg       0.45      0.44      0.44      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000), # We tried iter [1000-5000] but still the same result\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with Word2Vec\")\n",
    "    model.fit(X_w2v_train, y_train)\n",
    "    y_pred = model.predict(X_w2v_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#Random Forest based on the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0324209b-b584-4e44-b5a5-69552db3695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.64      0.67      0.65       276\n",
      "        fear       0.78      0.90      0.83       259\n",
      "         joy       0.60      0.30      0.40       268\n",
      "        love       0.66      0.70      0.68       250\n",
      "        none       0.47      0.91      0.62       307\n",
      "     sadness       0.51      0.26      0.34       258\n",
      "    surprise       0.71      0.26      0.38       201\n",
      "    sympathy       0.73      0.87      0.80       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.64      0.61      0.59      2013\n",
      "weighted avg       0.63      0.62      0.59      2013\n",
      "\n",
      "\n",
      "SVM (RBF Kernel) with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.60      0.58       276\n",
      "        fear       0.99      0.86      0.92       259\n",
      "         joy       0.36      0.50      0.42       268\n",
      "        love       0.77      0.51      0.61       250\n",
      "        none       0.57      0.96      0.72       307\n",
      "     sadness       0.51      0.24      0.32       258\n",
      "    surprise       0.60      0.32      0.42       201\n",
      "    sympathy       0.72      0.79      0.75       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.64      0.60      0.59      2013\n",
      "weighted avg       0.63      0.61      0.60      2013\n",
      "\n",
      "\n",
      "Decision Tree with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.52      0.52       276\n",
      "        fear       0.96      0.90      0.93       259\n",
      "         joy       0.33      0.28      0.30       268\n",
      "        love       0.64      0.48      0.55       250\n",
      "        none       0.62      0.73      0.67       307\n",
      "     sadness       0.31      0.44      0.37       258\n",
      "    surprise       0.43      0.38      0.40       201\n",
      "    sympathy       0.85      0.78      0.82       194\n",
      "\n",
      "    accuracy                           0.57      2013\n",
      "   macro avg       0.58      0.56      0.57      2013\n",
      "weighted avg       0.58      0.57      0.57      2013\n",
      "\n",
      "\n",
      "Random Forest with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.52      0.56       276\n",
      "        fear       0.96      0.91      0.93       259\n",
      "         joy       0.38      0.37      0.37       268\n",
      "        love       0.69      0.55      0.61       250\n",
      "        none       0.59      0.92      0.72       307\n",
      "     sadness       0.37      0.39      0.38       258\n",
      "    surprise       0.58      0.38      0.46       201\n",
      "    sympathy       0.87      0.86      0.86       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.63      0.61      0.61      2013\n",
      "weighted avg       0.62      0.62      0.61      2013\n",
      "\n",
      "\n",
      "AdaBoost with BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.22      0.90      0.35       276\n",
      "        fear       0.99      0.71      0.83       259\n",
      "         joy       0.00      0.00      0.00       268\n",
      "        love       0.94      0.19      0.31       250\n",
      "        none       0.56      0.97      0.71       307\n",
      "     sadness       0.00      0.00      0.00       258\n",
      "    surprise       0.00      0.00      0.00       201\n",
      "    sympathy       0.85      0.35      0.50       194\n",
      "\n",
      "    accuracy                           0.42      2013\n",
      "   macro avg       0.45      0.39      0.34      2013\n",
      "weighted avg       0.44      0.42      0.35      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} with BoW\")\n",
    "    model.fit(X_bow_train, y_train)\n",
    "    y_pred = model.predict(X_bow_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "#Random Forest based on the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73591232-72ad-4919-9a16-0334cdb56944",
   "metadata": {},
   "source": [
    "# **PART FOUR:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b18254-04f9-4cff-9a17-f8e2527c170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "227/227 [==============================] - 7s 12ms/step - loss: 1.6033 - accuracy: 0.4256 - val_loss: 1.1958 - val_accuracy: 0.6042\n",
      "Epoch 2/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.2513 - accuracy: 0.5610 - val_loss: 1.0598 - val_accuracy: 0.6377\n",
      "Epoch 3/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.1388 - accuracy: 0.6079 - val_loss: 1.0268 - val_accuracy: 0.6514\n",
      "Epoch 4/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.0768 - accuracy: 0.6289 - val_loss: 0.9980 - val_accuracy: 0.6625\n",
      "Epoch 5/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 1.0137 - accuracy: 0.6433 - val_loss: 0.9876 - val_accuracy: 0.6600\n",
      "Epoch 6/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.9675 - accuracy: 0.6686 - val_loss: 1.0072 - val_accuracy: 0.6563\n",
      "Epoch 7/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.9214 - accuracy: 0.6755 - val_loss: 0.9706 - val_accuracy: 0.6787\n",
      "Epoch 8/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8953 - accuracy: 0.6842 - val_loss: 1.0171 - val_accuracy: 0.6625\n",
      "Epoch 9/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8597 - accuracy: 0.7001 - val_loss: 0.9657 - val_accuracy: 0.6787\n",
      "Epoch 10/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8416 - accuracy: 0.7083 - val_loss: 0.9866 - val_accuracy: 0.6849\n",
      "Epoch 11/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.8076 - accuracy: 0.7161 - val_loss: 0.9873 - val_accuracy: 0.6712\n",
      "Epoch 12/30\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.7908 - accuracy: 0.7160 - val_loss: 1.0052 - val_accuracy: 0.6638\n",
      "63/63 [==============================] - 1s 4ms/step\n",
      "\n",
      "Feed-Forward NN (Keras) Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       276\n",
      "           1       0.93      0.88      0.91       259\n",
      "           2       0.57      0.45      0.50       268\n",
      "           3       0.73      0.69      0.71       250\n",
      "           4       0.56      0.82      0.66       307\n",
      "           5       0.56      0.45      0.50       258\n",
      "           6       0.48      0.41      0.44       201\n",
      "           7       0.82      0.82      0.82       194\n",
      "\n",
      "    accuracy                           0.66      2013\n",
      "   macro avg       0.66      0.65      0.65      2013\n",
      "weighted avg       0.66      0.66      0.65      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode the labels for multi-class output\n",
    "lb = LabelBinarizer()  # y_train contains labels\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Use Word2Vec averaged vectors as input\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(300,)))  # 300 = Word2Vec vector size\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))      # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_w2v_train, y_train_oh, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=le.classes_))'''\n",
    "\n",
    "##_____________________________________________________________________________________________________________________________##\n",
    "\n",
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode the labels for multi-class output\n",
    "lb = LabelBinarizer()\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Define the model with multiple hidden layers and dropout for regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='ReLU', input_shape=(300,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='ReLU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='ReLU'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # default learning rate ~0.001\n",
    "\n",
    "model.fit(X_w2v_train, y_train_oh, epochs=15, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=le.classes_))'''\n",
    "\n",
    "##_____________________________________________________________________________________________________________________________##\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_train_oh = lb.fit_transform(y_train)\n",
    "y_test_oh = lb.transform(y_test)\n",
    "\n",
    "# Optional: compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(300,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    X_w2v_train, y_train_oh,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ffnn = model.predict(X_w2v_test).argmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nFeed-Forward NN (Keras) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_ffnn, target_names=[str(cls) for cls in lb.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013830e-0fce-45ee-8a0f-f2a1b941816b",
   "metadata": {},
   "source": [
    "# **PART FIVE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3754ab9-2e29-4f79-b9d3-95858be0c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "202/202 [==============================] - 165s 526ms/step - loss: 1.3847 - accuracy: 0.5204 - val_loss: 1.1010 - val_accuracy: 0.6195\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 94s 467ms/step - loss: 0.9810 - accuracy: 0.6614 - val_loss: 1.0280 - val_accuracy: 0.6505\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 95s 468ms/step - loss: 0.8267 - accuracy: 0.7170 - val_loss: 1.0185 - val_accuracy: 0.6530\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 95s 469ms/step - loss: 0.7245 - accuracy: 0.7552 - val_loss: 1.0138 - val_accuracy: 0.6611\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 95s 472ms/step - loss: 0.6308 - accuracy: 0.7844 - val_loss: 1.0341 - val_accuracy: 0.6536\n",
      "63/63 [==============================] - 12s 168ms/step\n",
      "\n",
      "LSTM-Improved Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67       276\n",
      "           1       0.96      0.92      0.94       259\n",
      "           2       0.51      0.47      0.49       268\n",
      "           3       0.71      0.65      0.68       250\n",
      "           4       0.65      0.77      0.71       307\n",
      "           5       0.44      0.41      0.42       258\n",
      "           6       0.47      0.35      0.40       201\n",
      "           7       0.85      0.85      0.85       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.65      0.65      0.65      2013\n",
      "weighted avg       0.65      0.65      0.65      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "max_seq_len = 100\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_seq_len)\n",
    "\n",
    "# 2. Prepare embedding matrix from Word2Vec\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(5000, len(word_index) + 1)\n",
    "embedding_dim = w2v_model.vector_size  # usually 300\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# 3. One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_seq_len,\n",
    "    trainable=False  # freeze embeddings To preserve the original semantic information from high-quality \n",
    "                    # pre-trained embeddings like Word2Vec, GloVe, or AraVec & reduce training time \n",
    "                    # and prevent overfitting, especially with small datasets.\n",
    "))\n",
    "\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# 6. Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 7. Callbacks for early stopping and saving best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# 8. Train\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# 9. Predict & evaluate\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in lb.classes_]))\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Load your pre-trained Word2Vec model\n",
    "w2v_model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "# 2. Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "max_seq_len = 100\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_seq_len)\n",
    "\n",
    "# 3. Prepare embedding matrix using w2v_model\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(5000, len(word_index) + 1)\n",
    "embedding_dim = w2v_model.vector_size  # 300 for your CBOW model\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# 4. One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# 5. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_seq_len,\n",
    "    trainable=False \n",
    "))\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "# 7. Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# 9. Train model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# 10. Predict and evaluate\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\nLSTM-Improved Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in lb.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f0d42-0256-469c-92fa-7a514be78b10",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "715f05a4-aa68-4ecd-a0e9-35371fb908ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "101/101 [==============================] - 275s 2s/step - loss: 1.6226 - accuracy: 0.4248 - val_loss: 1.2487 - val_accuracy: 0.5829\n",
      "Epoch 2/5\n",
      "101/101 [==============================] - 175s 2s/step - loss: 1.1420 - accuracy: 0.6103 - val_loss: 1.0591 - val_accuracy: 0.6363\n",
      "Epoch 3/5\n",
      "101/101 [==============================] - 170s 2s/step - loss: 0.9841 - accuracy: 0.6636 - val_loss: 1.0375 - val_accuracy: 0.6555\n",
      "Epoch 4/5\n",
      "101/101 [==============================] - 169s 2s/step - loss: 0.8489 - accuracy: 0.7014 - val_loss: 1.0244 - val_accuracy: 0.6524\n",
      "Epoch 5/5\n",
      "101/101 [==============================] - 172s 2s/step - loss: 0.7666 - accuracy: 0.7339 - val_loss: 1.0616 - val_accuracy: 0.6605\n",
      "63/63 [==============================] - 16s 212ms/step\n",
      "\n",
      "BiLSTM Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67       276\n",
      "           1       0.94      0.89      0.92       259\n",
      "           2       0.53      0.44      0.48       268\n",
      "           3       0.70      0.66      0.68       250\n",
      "           4       0.59      0.85      0.70       307\n",
      "           5       0.50      0.40      0.45       258\n",
      "           6       0.50      0.35      0.41       201\n",
      "           7       0.84      0.85      0.84       194\n",
      "\n",
      "    accuracy                           0.65      2013\n",
      "   macro avg       0.65      0.65      0.64      2013\n",
      "weighted avg       0.65      0.65      0.64      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import zipfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load Pre-trained Word2Vec Model\n",
    "with zipfile.ZipFile('tweet_cbow_300.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('tweet_cbow_300')\n",
    "\n",
    "w2v_model = Word2Vec.load('tweet_cbow_300/tweets_cbow_300')\n",
    "embedding_dim = 300\n",
    "\n",
    "# Tokenize and Pad Sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=150)\n",
    "\n",
    "# Build Embedding Matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(10000, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Encode Labels\n",
    "lb = LabelBinarizer()\n",
    "y_oh = lb.fit_transform(y)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_pad, X_test_pad, y_train_oh, y_test_oh = train_test_split(X_pad, y_oh, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BiLSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=150,\n",
    "    trainable=False  \n",
    "))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training with Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_bilstm_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad, y_train_oh,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test_pad).argmax(axis=1)\n",
    "y_true = y_test_oh.argmax(axis=1)\n",
    "\n",
    "print(\"\\nBiLSTM Model Evaluation:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(cls) for cls in lb.classes_]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284be3d-4ec2-4f3c-99d1-2a41df718a5d",
   "metadata": {},
   "source": [
    "# **AraBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8aaf020-d2bf-4fbf-aa99-2300c23ec7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     81\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3fx4ebxh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1630\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1630\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ragha\\anaconda3\\envs\\MyEnv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(set(y))  # Replace `y` with your label list\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Tokenization\n",
    "def convert_examples(texts, labels):\n",
    "    return [\n",
    "        InputExample(guid=str(i), text_a=text, label=label)\n",
    "        for i, (text, label) in enumerate(zip(texts, labels))\n",
    "    ]\n",
    "\n",
    "def convert_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "\n",
    "    for e in examples:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        attention_mask = inputs[\"attention_mask\"][0]\n",
    "        features.append(({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }, e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield f\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "train_examples = convert_examples(X_train, y_train)\n",
    "test_examples = convert_examples(X_test, y_test)\n",
    "\n",
    "train_dataset = convert_to_tf_dataset(train_examples, tokenizer).shuffle(100).batch(16)\n",
    "test_dataset = convert_to_tf_dataset(test_examples, tokenizer).batch(16)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_probs = model.predict(test_dataset).logits\n",
    "y_pred = tf.argmax(y_pred_probs, axis=1).numpy()\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266ec8b-86be-4269-a9fe-596b31cfad30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
